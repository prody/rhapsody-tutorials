{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to prepare a dataset of human SAVs, with relative features and true labels, to be used for training a Rhapsody classifier.\n",
    "\n",
    "## List of human missense variants \n",
    "\n",
    "We start by importing lists of human SAVs, with relative pathogenicity assessments, compiled from the following publicly available datasets:\n",
    "* **HumVar, ExoVar, predictSNP, VariBench, SwissVar**, 5 datasets of labelled human missense variants already used in our  [previous publication](https://www.pnas.org/content/115/16/4164) \n",
    "* **Humsavar**, a database of \"human polymorphisms and disease mutations\" available on [Uniprot](https://www.uniprot.org/docs/humsavar)\n",
    "* **ClinVar** a \"public archive of reports of the relationships among human variations and phenotypes, with supporting evidence\" [(FTP site)](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/). It contains a *review score*, based on a ranking out of 4 stars, that allows us to test performances on different levels of \"confidence in the accuracy of variation calls and assertions of clinical significance\".\n",
    "\n",
    "When combining these datasets together, SAVs with discordant interpretation are assigned with a `true_label = -1`.\n",
    "\n",
    "The resulting dataset is called **Integrated Dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# extract data\n",
    "if not os.path.isdir('local'):\n",
    "    os.mkdir('local')\n",
    "if not os.path.isdir('local/data'):\n",
    "    tar = tarfile.open('data.tar.gz', \"r:gz\")\n",
    "    tar.extractall(path='local')\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy structured array containing list of SAVs\n",
    "ID = np.load('local/data/Integrated_Dataset-SAVs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated Dataset size: 94505\n"
     ]
    }
   ],
   "source": [
    "print('Integrated Dataset size:', len(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SAV_coords', 'true_label', 'datasets', 'ClinVar_review_star')\n",
      "('A0AV02 181 R C', 0, 'humsavar[0],swissvar[0]', -1)\n"
     ]
    }
   ],
   "source": [
    "print(ID.dtype.names)\n",
    "print(ID[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, -1}\n"
     ]
    }
   ],
   "source": [
    "# true labels: 0 (neutral), 1 (deleterious), -1 (unknown or discordant interpretations)\n",
    "print( set(ID['true_label']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVs imported from ClinVar are assigned with a review star (see this [link](https://www.ncbi.nlm.nih.gov/clinvar/docs/review_status) for meaning). If a SAV was not found in ClinVar and does not have a review star, we will set its value to `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, -1}\n"
     ]
    }
   ],
   "source": [
    "print( set(ID['ClinVar_review_star']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           clinvar         exovar          humsavar        humvar          predictSNP      swissvar        varibench      \n",
      "clinvar    20814(89)/61.4% \n",
      "exovar     2984(475)       8809/58.5%      \n",
      "humsavar   19043(2729)     5098(341)       68386/42.4%     \n",
      "humvar     12429(2222)     5438(21)        37003(1005)     40177/52.3%     \n",
      "predictSNP 299(21)         8               3222(48)        25(1)           10459(1)/62.6%  \n",
      "swissvar   1119(127)       10              7058(60)        56(1)           21(1)           8862(1)/31.7%   \n",
      "varibench  665(291)        1               697(66)         14              2               4               10237/42.1%     "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "list_of_datasets = ['clinvar', 'exovar', 'humsavar', 'humvar', 'predictSNP', 'swissvar', 'varibench']\n",
    "print(' '*11 + ' '.join([f'{d:15}' for d in list_of_datasets]))\n",
    "\n",
    "for i, d_i in enumerate(list_of_datasets):\n",
    "    print(f'{d_i:11}', end='')\n",
    "    for j, d_j in enumerate(list_of_datasets):\n",
    "        if j < i:\n",
    "            shared_SAVs = [s for s in ID['datasets'] if d_i in s and d_j in s]\n",
    "            d_i_tlabels = [re.findall(d_i + '\\[.\\]', s)[0].split('[')[1][:-1] for s in shared_SAVs]\n",
    "            d_j_tlabels = [re.findall(d_j + '\\[.\\]', s)[0].split('[')[1][:-1] for s in shared_SAVs]\n",
    "            n_discordant = sum(np.array(d_i_tlabels) != np.array(d_j_tlabels))\n",
    "            if n_discordant:\n",
    "                summary = f'{len(shared_SAVs)}({n_discordant})'\n",
    "            else:\n",
    "                summary = f'{len(shared_SAVs)}'\n",
    "            print(f'{summary:<15}', end=' ')\n",
    "        elif j == i:\n",
    "            SAVs = [s for s in ID['datasets'] if d_i in s]\n",
    "            tl_count = Counter([re.findall(d_j + '\\[.\\]', s)[0].split('[')[1][:-1] for s in SAVs])\n",
    "            n_ambiguous  = tl_count['?']\n",
    "            dataset_bias = 100*tl_count['1']/(tl_count['0'] + tl_count['1'])\n",
    "            if n_ambiguous:\n",
    "                summary = f'{len(SAVs)}({n_ambiguous})/{dataset_bias:3.1f}%'\n",
    "            else:\n",
    "                summary = f'{len(SAVs)}/{dataset_bias:3.1f}%'\n",
    "            print(f'{summary:<15}', end=' ')    \n",
    "        else:\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parentheses: SAVs with discordant clinical interpretations between the two datasets.\n",
    "On the diagonal, the *dataset bias* (percentage of positive cases) is also reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Rhapsody features\n",
    "Precomputed features can be found in `local/data/` (computing them from scratch for the complete dataset takes a couple of days).\n",
    "In the following, we show how to compute Rhapsody features for a small set of SAVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, insert here local path to Rhapsody folder with the command:\n",
    "# sys.path.insert(0, '/LOCAL_PATH/rhapsody/')\n",
    "import rhapsody as rhaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SAVs = ['O00294 496 A T', 'O00238 31 R H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Local Rhapsody folder is set: '/home/lponzoni/Scratch/028-RHAPSODY-git/rhapsody-tutorials/paper_Supplementary_Info/00-Training_Dataset/local/pickles'\n"
     ]
    }
   ],
   "source": [
    "# set folder where pickles used to speed up calculations will be stored\n",
    "if not os.path.isdir('local/pickles'):\n",
    "    os.mkdir('local/pickles')\n",
    "rhaps.pathRhapsodyFolder('local/pickles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precomputed \"mutation effects\" from EVmutation can be downloaded at this [link](https://marks.hms.harvard.edu/evmutation/human_proteins.html) (~460 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Local EVmutation folder is set: '/home/lponzoni/Data/025-EVmutation/mutation_effects'\n"
     ]
    }
   ],
   "source": [
    "# Insert here path to folder with EVmutation precomputed mutation effects\n",
    "rhaps.pathEVmutationFolder('/home/lponzoni/Data/025-EVmutation/mutation_effects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('local/results'):\n",
    "    os.mkdir('local/results')\n",
    "os.chdir('local/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a rhapsody object\n",
    "rh = rhaps.Rhapsody()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> PolyPhen-2's output parsed.\n"
     ]
    }
   ],
   "source": [
    "# import SAVs by querying PolyPhen-2 (or by importing precomputed PolyPhen-2 output file, if found)\n",
    "if os.path.isfile('pph2-full.txt'):\n",
    "    rh.importPolyPhen2output('pph2-full.txt')\n",
    "else:\n",
    "    rh.queryPolyPhen2(test_SAVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would like to compute all features\n",
    "rh.setFeatSet('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# true labels must be imported prior to exporting training data\n",
    "true_labels = {\n",
    "    'O00294 496 A T': 1,\n",
    "    'O00238 31 R H': 0\n",
    "}\n",
    "rh.setTrueLabels(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Sequence-conservation features have been retrieved from PolyPhen-2's output.\n",
      "@> Mapping SAVs to PDB structures...\n",
      "@> [1/2] Mapping SAV 'O00238 31 R H' to PDB...\n",
      "@> Pickle 'UniprotMap-O00238.pkl' recovered.\n",
      "@> [2/2] Mapping SAV 'O00294 496 A T' to PDB...\n",
      "@> Pickle 'UniprotMap-O00238.pkl' saved.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' recovered.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' saved.\n",
      "@> SAVs have been mapped to PDB in 0.1s.\n",
      "@> Computing structural and dynamical features from PDB structures...\n",
      "@> [2/2] Analizing mutation site 2FIM:A 443...\n",
      "@> Pickle 'PDBfeatures-2FIM.pkl' recovered.\n",
      "@> Pickle 'PDBfeatures-2FIM.pkl' saved.\n",
      "@> PDB features have been computed in 0.0s.\n",
      "@> Computing sequence properties from Pfam domains...\n",
      "@> [1/2] Mapping SAV 'O00238 31 R H' to Pfam...\n",
      "@> Pickle 'UniprotMap-O00238.pkl' recovered.\n",
      "@> [2/2] Mapping SAV 'O00294 496 A T' to Pfam...\n",
      "@> Pickle 'UniprotMap-O00238.pkl' saved.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' recovered.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' saved.\n",
      "@> SAVs have been mapped on Pfam domains and sequence properties have been computed in 0.0s.\n",
      "@> Recovering EVmutation data...\n",
      "@> EVmutation scores recovered in 0.0s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([('O00294 496 A T', '2FIM A 443 A', 224, 1, 0.41869268, 0.2725254, 0.30990002, 0.00208266, 0.00289542, 0.0444308, 0.00296154, 0.00206255, 0.04027145, 0., 0.754,  1., -3.1479, -1.0065, 0.0662341, 0.3341, 0.09581726, 0.08719353, 0.08953744, 0.00398543, 0.00332497, 0.01278353, 0.00573205, 0.00480731, 0.01438932, 79., 78., 2.1440704, 0.813278  , 12.638108, 14.42894, 14.559032, -2.376),\n",
       "       ('O00238 31 R H', 'Unable to map SAV to PDB',   0, 0,        nan,       nan,        nan,        nan,        nan,       nan,        nan,        nan,        nan, 0., 1.634, nan, -2.4718, -2.461 , 0.0102818, 0.3508,        nan,        nan,        nan,        nan,        nan,        nan,        nan,        nan,        nan, nan, nan, 1.8702421, 0.23376623,       nan,      nan,       nan, -2.769)],\n",
       "      dtype=[('SAV_coords', '<U50'), ('Uniprot2PDB', '<U100'), ('PDB_length', '<i2'), ('true_label', '<i2'), ('ANM_MSF-chain', '<f4'), ('ANM_MSF-reduced', '<f4'), ('ANM_MSF-sliced', '<f4'), ('ANM_effectiveness-chain', '<f4'), ('ANM_effectiveness-reduced', '<f4'), ('ANM_effectiveness-sliced', '<f4'), ('ANM_sensitivity-chain', '<f4'), ('ANM_sensitivity-reduced', '<f4'), ('ANM_sensitivity-sliced', '<f4'), ('BLOSUM', '<f4'), ('Delta_PSIC', '<f4'), ('Delta_SASA', '<f4'), ('EVmut-DeltaE_epist', '<f4'), ('EVmut-DeltaE_indep', '<f4'), ('EVmut-mut_aa_freq', '<f4'), ('EVmut-wt_aa_cons', '<f4'), ('GNM_MSF-chain', '<f4'), ('GNM_MSF-reduced', '<f4'), ('GNM_MSF-sliced', '<f4'), ('GNM_effectiveness-chain', '<f4'), ('GNM_effectiveness-reduced', '<f4'), ('GNM_effectiveness-sliced', '<f4'), ('GNM_sensitivity-chain', '<f4'), ('GNM_sensitivity-reduced', '<f4'), ('GNM_sensitivity-sliced', '<f4'), ('SASA', '<f4'), ('SASA_in_complex', '<f4'), ('entropy', '<f4'), ('ranked_MI', '<f4'), ('stiffness-chain', '<f4'), ('stiffness-reduced', '<f4'), ('stiffness-sliced', '<f4'), ('wt_PSIC', '<f4')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = rh.exportTrainingData()\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('precomputed_features', training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a custom classifier\n",
    "\n",
    "This tutorial provides step-by-step instructions for building a new classifier on a personalized set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('local'):\n",
    "    os.mkdir('local')\n",
    "if not os.path.isdir('local/custom_classifier'):\n",
    "    os.mkdir('local/custom_classifier')\n",
    "    \n",
    "os.chdir('local/custom_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training dataset\n",
    "First, we need to build a custom training dataset, for instance by extending the Integrated Dataset that comes with Rhapsody. \n",
    "\n",
    "New feature arrays must have the same length as those already in the Integrated Dataset. Information about size and composition of the default training dataset is stored in the installation settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rhapsody as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> rhapsody_local_folder   : /home/luca/rhapsody\n",
      "@> EVmutation_local_folder : /home/luca/rhapsody/EVmutation_mutation_effects\n",
      "@> full classifier         : /home/luca/rhapsody/default_classifiers-sklearn_v0.21.3/full/trained_classifier.pkl\n",
      "@> reduced classifier      : /home/luca/rhapsody/default_classifiers-sklearn_v0.21.3/reduced/trained_classifier.pkl\n",
      "@> EVmut classifier        : /home/luca/rhapsody/default_classifiers-sklearn_v0.21.3/EVmut/trained_classifier.pkl\n",
      "@> training dataset size   : 20361\n",
      "@> EVmutation_metrics      : <computed>\n"
     ]
    }
   ],
   "source": [
    "settings = rd.getSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20361"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings['rhapsody_training_dataset']['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SAV_coords',\n",
       " 'Uniprot2PDB',\n",
       " 'PDB_length',\n",
       " 'true_label',\n",
       " 'ANM_MSF-chain',\n",
       " 'ANM_MSF-reduced',\n",
       " 'ANM_MSF-sliced',\n",
       " 'ANM_effectiveness-chain',\n",
       " 'ANM_effectiveness-reduced',\n",
       " 'ANM_effectiveness-sliced',\n",
       " 'ANM_sensitivity-chain',\n",
       " 'ANM_sensitivity-reduced',\n",
       " 'ANM_sensitivity-sliced',\n",
       " 'BLOSUM',\n",
       " 'Delta_PSIC',\n",
       " 'Delta_SASA',\n",
       " 'EVmut-DeltaE_epist',\n",
       " 'EVmut-DeltaE_indep',\n",
       " 'EVmut-mut_aa_freq',\n",
       " 'EVmut-wt_aa_cons',\n",
       " 'GNM_MSF-chain',\n",
       " 'GNM_MSF-reduced',\n",
       " 'GNM_MSF-sliced',\n",
       " 'GNM_effectiveness-chain',\n",
       " 'GNM_effectiveness-reduced',\n",
       " 'GNM_effectiveness-sliced',\n",
       " 'GNM_sensitivity-chain',\n",
       " 'GNM_sensitivity-reduced',\n",
       " 'GNM_sensitivity-sliced',\n",
       " 'SASA',\n",
       " 'SASA_in_complex',\n",
       " 'entropy',\n",
       " 'ranked_MI',\n",
       " 'stiffness-chain',\n",
       " 'stiffness-reduced',\n",
       " 'stiffness-sliced',\n",
       " 'wt_PSIC')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings['rhapsody_training_dataset']['fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's add a new field `PDB_length`, extracted from the Integrated Dataset, to the \"full\" Rhapsody training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds = rd.getDefaultTrainingDataset()\n",
    "array = default_ds['PDB_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array already has the correct length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20361"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this extra field `'new_feature'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([('A0FGR8 210 C S', 0, -2.053, 0.305, 13., 1.6369812 , 0.47203878, 0.48780644, 10.934773 , 1.9736528, 0.7888889, -1., 455.),\n",
       "           ('A0FGR8 638 S G', 0, -2.261, 1.017, 70., 1.1408623 , 0.6298156 , 0.44004798, 11.919116 ,       nan,       nan,  0., 455.),\n",
       "           ('A1Z1Q3 58 T I', 0, -1.815, 1.578, 85., 4.432677  , 0.0067604 , 0.6147862 , 10.329021 ,       nan,       nan, -1., 221.),\n",
       "           ...,\n",
       "           ('Q9Y6X9 283 R H', 0, -1.281, 2.726, 30., 0.474798  , 0.48984432, 0.19532807, 13.609769 ,       nan,       nan,  0., 536.),\n",
       "           ('Q9Y6X9 466 D H', 0, -1.261, 3.143, 71., 0.46281278, 0.45384642, 0.16229136, 13.8006115,       nan,       nan, -1., 536.),\n",
       "           ('Q9Y6X9 87 S L', 1, -1.239, 1.896, 17., 0.4945855 , 0.57216614, 0.23604834, 13.548991 , 1.1867005, 0.5555556, -2., 536.)],\n",
       "          dtype=[('SAV_coords', '<U50'), ('true_label', '<i2'), ('wt_PSIC', '<f4'), ('Delta_PSIC', '<f4'), ('SASA', '<f4'), ('ANM_MSF-chain', '<f4'), ('ANM_effectiveness-chain', '<f4'), ('ANM_sensitivity-chain', '<f4'), ('stiffness-chain', '<f4'), ('entropy', '<f4'), ('ranked_MI', '<f4'), ('BLOSUM', '<f4'), ('new_feature', '<f4')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_training_dataset = rd.extendDefaultTrainingDataset('new_feature', array)\n",
    "new_training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple new features can be added at once by providing a list of arrays:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rd.extendDefaultTrainingDataset(['new_feature_1', 'new_feature_2', ...], [array1, array2, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A completely different training dataset can also be built from scratch, by creating a NumPy structured array, containing the two mandatory fields `SAV_coords` and `true_label`.\n",
    "\n",
    "Finally, let's train the new custom classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> 3918 out of 20361 cases ignored with missing features.\n",
      "@> CV iteration # 1:   AUROC = 0.871   AUPRC = 0.931   OOB score = 0.820\n",
      "@> CV iteration # 2:   AUROC = 0.875   AUPRC = 0.934   OOB score = 0.820\n",
      "@> CV iteration # 3:   AUROC = 0.867   AUPRC = 0.926   OOB score = 0.820\n",
      "@> CV iteration # 4:   AUROC = 0.878   AUPRC = 0.934   OOB score = 0.818\n",
      "@> CV iteration # 5:   AUROC = 0.872   AUPRC = 0.935   OOB score = 0.821\n",
      "@> CV iteration # 6:   AUROC = 0.875   AUPRC = 0.935   OOB score = 0.820\n",
      "@> CV iteration # 7:   AUROC = 0.871   AUPRC = 0.931   OOB score = 0.819\n",
      "@> CV iteration # 8:   AUROC = 0.873   AUPRC = 0.934   OOB score = 0.821\n",
      "@> CV iteration # 9:   AUROC = 0.861   AUPRC = 0.922   OOB score = 0.820\n",
      "@> CV iteration #10:   AUROC = 0.879   AUPRC = 0.935   OOB score = 0.818\n",
      "@> ------------------------------------------------------------\n",
      "@> Cross-validation summary:\n",
      "@> training dataset size:   16443\n",
      "@> fraction of positives:   0.690\n",
      "@> mean AUROC:              0.872 +/- 0.005\n",
      "@> mean AUPRC:              0.932 +/- 0.004\n",
      "@> mean OOB score:          0.820 +/- 0.001\n",
      "@> optimal cutoff*:         0.709 +/- 0.036\n",
      "@> (* argmax of Youden's index)\n",
      "@> feature importances:\n",
      "@>                 wt_PSIC: 0.143\n",
      "@>              Delta_PSIC: 0.188\n",
      "@>                    SASA: 0.068\n",
      "@>           ANM_MSF-chain: 0.074\n",
      "@> ANM_effectiveness-chain: 0.078\n",
      "@>   ANM_sensitivity-chain: 0.071\n",
      "@>         stiffness-chain: 0.080\n",
      "@>                 entropy: 0.099\n",
      "@>               ranked_MI: 0.068\n",
      "@>                  BLOSUM: 0.047\n",
      "@>             new_feature: 0.084\n",
      "@> ------------------------------------------------------------\n",
      "@> Predictions distribution saved to predictions_distribution.png\n",
      "@> Pathogenicity plot saved to pathogenicity_prob.png\n",
      "@> ROC plot saved to ROC.png\n",
      "@> ------------------------------------------------------------\n",
      "@> Classifier training summary:\n",
      "@> mean OOB score:          0.820\n",
      "@> feature importances:\n",
      "@>                 wt_PSIC: 0.144\n",
      "@>              Delta_PSIC: 0.188\n",
      "@>                    SASA: 0.069\n",
      "@>           ANM_MSF-chain: 0.074\n",
      "@> ANM_effectiveness-chain: 0.078\n",
      "@>   ANM_sensitivity-chain: 0.071\n",
      "@>         stiffness-chain: 0.080\n",
      "@>                 entropy: 0.098\n",
      "@>               ranked_MI: 0.068\n",
      "@>                  BLOSUM: 0.047\n",
      "@>             new_feature: 0.084\n",
      "@> ------------------------------------------------------------\n",
      "@> Feat. importance plot saved to feat_importances.png\n"
     ]
    }
   ],
   "source": [
    "new_clsf_info = rd.trainRFclassifier(new_training_dataset, pickle_name='custom_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Note that, although its introduction yields an increased accuracy, the PDB size is not a good feature and probably reflects an intrinsic bias of the training dataset (more deleterious SAVs have been reported for  proteins of larger size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new custom classifier accuracy\n",
    "new_clsf_info['CV summary']['mean AUROC']\n",
    "del new_clsf_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "full_clsf_fname = rd.getDefaultClassifiers()['full']\n",
    "full_clsf = pickle.load(open(full_clsf_fname, 'rb'))\n",
    "\n",
    "# \"full\" Rhapsody classifier accuracy\n",
    "full_clsf['CV summary']['mean AUROC']\n",
    "del full_clsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting predictions based on custom classifier\n",
    "Here we show how to obtain pathogenicity predictions for a small set of 5 Single Amino acid Variants, using our new custom classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SAVs = [\n",
    "    'O00294 496 A T',  # know neutral SAV used for training\n",
    "    'O00238 31 R H',   # SAV with no PDB structure\n",
    "    'P01112 58 T R',   # unknown SAV\n",
    "    'P01112 30 D E',   # unknown SAV\n",
    "    'P01112 170 K I',  # unknown SAV with no Pfam domain\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot simply use the main interface `rhapsody()`, because the new feature cannot be computed automatically by Rhapsody:\n",
    "```\n",
    "# This would cause an error\n",
    "rh = rd.rhapsody(test_SAVs, main_classifier='custom_classifier.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Rhapsody` class contains all the necessary functionalities for getting custom predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Submitting query to PolyPhen-2...\n",
      "@> Query to PolyPhen-2 started in 1.7s.\n",
      "@> PolyPhen-2 is running...\n",
      "@> Query to PolyPhen-2 completed in 20.0s.\n",
      "@> PolyPhen-2's output parsed.\n"
     ]
    }
   ],
   "source": [
    "# initialize a new instance by importing SAVs and querying PolyPhen-2 directly\n",
    "rh = rd.Rhapsody(query=test_SAVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import new precomputed features\n",
    "features_dict = {\n",
    "    'new_feature': [224,   0, 168, 168, 171]\n",
    "}\n",
    "rh.importPrecomputedExtraFeatures(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Imported feature set:\n",
      "@>    'wt_PSIC' \n",
      "@>    'Delta_PSIC' \n",
      "@>    'SASA' \n",
      "@>    'ANM_MSF-chain' \n",
      "@>    'ANM_effectiveness-chain' \n",
      "@>    'ANM_sensitivity-chain' \n",
      "@>    'stiffness-chain' \n",
      "@>    'entropy' \n",
      "@>    'ranked_MI' \n",
      "@>    'BLOSUM' \n",
      "@>    'new_feature' \n"
     ]
    }
   ],
   "source": [
    "# import the classifier that will be used for predictions\n",
    "rh.importClassifiers('custom_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Sequence-conservation features have been retrieved from PolyPhen-2's output.\n",
      "@> Mapping SAVs to PDB structures...\n",
      "Mapping SAV 'O00238 31 R H' to PDB:   0%|          | 0/5 [00:00<?]@> Pickle 'UniprotMap-O00238.pkl' recovered.\n",
      "Mapping SAV 'O00294 496 A T' to PDB:  20%|██        | 1/5 [00:00<00:00]@> Pickle 'UniprotMap-O00238.pkl' saved.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' recovered.\n",
      "Mapping SAV 'P01112 58 T R' to PDB:  40%|████      | 2/5 [00:00<00:00] @> Pickle 'UniprotMap-O00294.pkl' saved.\n",
      "@> Pickle 'UniprotMap-P01112.pkl' recovered.\n",
      "Mapping SAV 'P01112 170 K I' to PDB: 100%|██████████| 5/5 [00:02<00:00]\n",
      "@> Pickle 'UniprotMap-P01112.pkl' saved.\n",
      "@> 4 out of 5 SAVs have been mapped to PDB in 2.3s.\n",
      "@> Computing structural and dynamical features from PDB structures...\n",
      "Analizing mutation site 1AA9:A 170:   0%|          | 0/5 [00:00<?]@> Pickle 'PDBfeatures-1AA9.pkl' recovered.\n",
      "Analizing mutation site 2FIM:A 443:   0%|          | 0/5 [00:00<?]@> Pickle 'PDBfeatures-1AA9.pkl' saved.\n",
      "@> Pickle 'PDBfeatures-2FIM.pkl' recovered.\n",
      "Analizing mutation site 4Q21:A 58:   0%|          | 0/5 [00:00<?] @> Pickle 'PDBfeatures-2FIM.pkl' saved.\n",
      "@> Pickle 'PDBfeatures-4Q21.pkl' recovered.\n",
      "Analizing mutation site 4Q21:A 30:   0%|          | 0/5 [00:00<?]@> Pickle 'PDBfeatures-4Q21.pkl' saved.\n",
      "Analizing mutation site 4Q21:A 30: 100%|██████████| 5/5 [00:00<00:00]\n",
      "@> PDB features have been computed in 0.0s.\n",
      "@> Computing sequence properties from Pfam domains...\n",
      "Mapping SAV 'O00238 31 R H' to Pfam:   0%|          | 0/5 [00:00<?]@> Pickle 'UniprotMap-O00238.pkl' recovered.\n",
      "Mapping SAV 'O00294 496 A T' to Pfam:   0%|          | 0/5 [00:00<?]@> Pickle 'UniprotMap-O00238.pkl' saved.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' recovered.\n",
      "Mapping SAV 'P01112 58 T R' to Pfam:   0%|          | 0/5 [00:00<?] @> Pickle 'UniprotMap-O00294.pkl' saved.\n",
      "@> Pickle 'UniprotMap-P01112.pkl' recovered.\n",
      "Mapping SAV 'P01112 170 K I' to Pfam:  60%|██████    | 3/5 [00:00<00:00]@> WARNING Unable to compute Pfam features: No Pfam domain for resid 170.\n",
      "@> Pickle 'UniprotMap-P01112.pkl' saved.\n",
      "Mapping SAV 'P01112 170 K I' to Pfam: 100%|██████████| 5/5 [00:00<00:00]\n",
      "@> SAVs have been mapped on Pfam domains and sequence properties have been computed in 0.4s.\n",
      "@> Random Forest classifier imported in 22.6s.\n",
      "@> 3 predictions computed in 0.5s.\n",
      "@> Recovering EVmutation data...\n",
      "@> EVmutation scores recovered in 0.5s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([('O00294 496 A T', 'known_neu', 0.086     , 0.03065213, 'neutral', 0.351, 'neutral', -3.1479, 'neutral'),\n",
       "       ('O00238 31 R H', 'new',        nan,        nan, '?', 0.219, 'neutral', -2.4718, 'neutral'),\n",
       "       ('P01112 58 T R', 'new', 0.956     , 0.913615  , 'deleterious', 1.   , 'deleterious', -9.7604, 'deleterious'),\n",
       "       ('P01112 30 D E', 'new', 0.12666667, 0.044859  , 'neutral', 0.001, 'neutral',  0.2196, 'neutral'),\n",
       "       ('P01112 170 K I', 'new',        nan,        nan, '?', 0.   , 'neutral',     nan, '?')],\n",
       "      dtype=[('SAV coords', '<U50'), ('training info', '<U12'), ('score', '<f4'), ('path. prob.', '<f4'), ('path. class', '<U12'), ('PolyPhen-2 score', '<f4'), ('PolyPhen-2 path. class', '<U12'), ('EVmutation score', '<f4'), ('EVmutation path. class', '<U12')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh.getPredictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to prepare a dataset of human SAVs, with relative features and true labels, to be used for training a Rhapsody classifier.\n",
    "\n",
    "## List of human missense variants \n",
    "\n",
    "We start by importing lists of human SAVs, with relative pathogenicity assessments, compiled from the following publicly available datasets:\n",
    "* **HumVar, ExoVar, predictSNP, VariBench, SwissVar**, 5 datasets of labelled human missense variants already used in our  [previous publication](https://www.pnas.org/content/115/16/4164) \n",
    "* **Humsavar**, a database of \"human polymorphisms and disease mutations\" available on [Uniprot](https://www.uniprot.org/docs/humsavar)\n",
    "* **ClinVar** a \"public archive of reports of the relationships among human variations and phenotypes, with supporting evidence\" [(FTP site)](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/). It contains a *review score*, based on a ranking out of 4 stars, that allows us to test performances on different levels of \"confidence in the accuracy of variation calls and assertions of clinical significance\".\n",
    "\n",
    "When combining these datasets together, SAVs with discordant interpretation are assigned with a `true_label = -1`.\n",
    "\n",
    "The resulting dataset is called **Integrated Dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# extract data\n",
    "if not os.path.isdir('local'):\n",
    "    os.mkdir('local')\n",
    "if not os.path.isdir('local/data'):\n",
    "    tar = tarfile.open('data.tar.gz', \"r:gz\")\n",
    "    tar.extractall(path='local')\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy structured array containing list of SAVs\n",
    "ID = np.load('local/data/Integrated_Dataset-SAVs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated Dataset size: 94505\n"
     ]
    }
   ],
   "source": [
    "print('Integrated Dataset size:', len(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SAV_coords', 'true_label', 'datasets', 'ClinVar_review_star')\n",
      "('A0AV02 181 R C', 0, 'humsavar[0],swissvar[0]', -1)\n"
     ]
    }
   ],
   "source": [
    "print(ID.dtype.names)\n",
    "print(ID[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, -1}\n"
     ]
    }
   ],
   "source": [
    "# true labels: 0 (neutral), 1 (deleterious), -1 (unknown or discordant interpretations)\n",
    "print( set(ID['true_label']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVs imported from ClinVar are assigned with a review star (see this [link](https://www.ncbi.nlm.nih.gov/clinvar/docs/review_status) for meaning). If a SAV was not found in ClinVar and does not have a review star, we will set its value to `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, -1}\n"
     ]
    }
   ],
   "source": [
    "print( set(ID['ClinVar_review_star']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           clinvar         exovar          humsavar        humvar          predictSNP      swissvar        varibench      \n",
      "clinvar    20814(89)/61.4% \n",
      "exovar     2984(475)       8809/58.5%      \n",
      "humsavar   19043(2729)     5098(341)       68386/42.4%     \n",
      "humvar     12429(2222)     5438(21)        37003(1005)     40177/52.3%     \n",
      "predictSNP 299(21)         8               3222(48)        25(1)           10459(1)/62.6%  \n",
      "swissvar   1119(127)       10              7058(60)        56(1)           21(1)           8862(1)/31.7%   \n",
      "varibench  665(291)        1               697(66)         14              2               4               10237/42.1%     "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "list_of_datasets = ['clinvar', 'exovar', 'humsavar', 'humvar', 'predictSNP', 'swissvar', 'varibench']\n",
    "print(' '*11 + ' '.join([f'{d:15}' for d in list_of_datasets]))\n",
    "\n",
    "for i, d_i in enumerate(list_of_datasets):\n",
    "    print(f'{d_i:11}', end='')\n",
    "    for j, d_j in enumerate(list_of_datasets):\n",
    "        if j < i:\n",
    "            shared_SAVs = [s for s in ID['datasets'] if d_i in s and d_j in s]\n",
    "            d_i_tlabels = [re.findall(d_i + '\\[.\\]', s)[0].split('[')[1][:-1] for s in shared_SAVs]\n",
    "            d_j_tlabels = [re.findall(d_j + '\\[.\\]', s)[0].split('[')[1][:-1] for s in shared_SAVs]\n",
    "            n_discordant = sum(np.array(d_i_tlabels) != np.array(d_j_tlabels))\n",
    "            if n_discordant:\n",
    "                summary = f'{len(shared_SAVs)}({n_discordant})'\n",
    "            else:\n",
    "                summary = f'{len(shared_SAVs)}'\n",
    "            print(f'{summary:<15}', end=' ')\n",
    "        elif j == i:\n",
    "            SAVs = [s for s in ID['datasets'] if d_i in s]\n",
    "            tl_count = Counter([re.findall(d_j + '\\[.\\]', s)[0].split('[')[1][:-1] for s in SAVs])\n",
    "            n_ambiguous  = tl_count['?']\n",
    "            dataset_bias = 100*tl_count['1']/(tl_count['0'] + tl_count['1'])\n",
    "            if n_ambiguous:\n",
    "                summary = f'{len(SAVs)}({n_ambiguous})/{dataset_bias:3.1f}%'\n",
    "            else:\n",
    "                summary = f'{len(SAVs)}/{dataset_bias:3.1f}%'\n",
    "            print(f'{summary:<15}', end=' ')    \n",
    "        else:\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parentheses: SAVs with discordant clinical interpretations between the two datasets.\n",
    "On the diagonal, the *dataset bias* (percentage of positive cases) is also reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Rhapsody features\n",
    "Precomputed features can be found in `local/data/` (computing them from scratch for the complete dataset takes a couple of days).\n",
    "In the following, we show how to compute Rhapsody features for a small set of SAVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, insert here local path to Rhapsody folder with the command:\n",
    "# sys.path.insert(0, '/LOCAL_PATH/rhapsody/')\n",
    "import rhapsody as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SAVs = ['O00294 496 A T', 'O00238 31 R H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('local/results'):\n",
    "    os.mkdir('local/results')\n",
    "os.chdir('local/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a rhapsody object\n",
    "rh = rd.Rhapsody()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> PolyPhen-2's output parsed.\n"
     ]
    }
   ],
   "source": [
    "# import SAVs by querying PolyPhen-2 (or by importing precomputed PolyPhen-2 output file, if found)\n",
    "if not os.path.isfile('pph2-full.txt'):\n",
    "    rh.queryPolyPhen2(test_SAVs)\n",
    "else:\n",
    "    rh.importPolyPhen2output('pph2-full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would like to compute all features\n",
    "rh.setFeatSet('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# true labels must be imported prior to exporting training data\n",
    "true_labels = {\n",
    "    'O00294 496 A T': 1,\n",
    "    'O00238 31 R H': 0\n",
    "}\n",
    "rh.setTrueLabels(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Sequence-conservation features have been retrieved from PolyPhen-2's output.\n",
      "@> Mapping SAVs to PDB structures...\n",
      "Mapping SAV 'O00238 31 R H' to PDB:   0%|          | 0/2 [00:00<?]@> Pickle 'UniprotMap-O00238.pkl' recovered.\n",
      "Mapping SAV 'O00294 496 A T' to PDB:  50%|█████     | 1/2 [00:00<00:00]@> Pickle 'UniprotMap-O00238.pkl' saved.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' recovered.\n",
      "Mapping SAV 'O00294 496 A T' to PDB: 100%|██████████| 2/2 [00:00<00:00]\n",
      "@> Pickle 'UniprotMap-O00294.pkl' saved.\n",
      "@> 1 out of 2 SAVs have been mapped to PDB in 0.2s.\n",
      "@> Computing structural and dynamical features from PDB structures...\n",
      "Analizing mutation site 2FIM:A 443:   0%|          | 0/2 [00:00<?]@> Pickle 'PDBfeatures-2FIM.pkl' recovered.\n",
      "@> PDB file is found in the local folder (/home/luca/.../2fim.pdb.gz).\n",
      "@> 3841 atoms and 1 coordinate set(s) were parsed in 0.09s.\n",
      "@> Running DSSP...\n",
      "@> DSSP finished in 3.6s.\n",
      "@> Kirchhoff was built in 0.01s.\n",
      "@> 223 modes were calculated in 0.37s.\n",
      "@> Calculating covariance matrix\n",
      "@> Covariance matrix calculated in 0.0s.\n",
      "@> Calculating perturbation response\n",
      "@> Perturbation response matrix calculated in 0.1s.\n",
      "@> Perturbation response scanning completed in 0.1s.\n",
      "@> Kirchhoff was built in 0.03s.\n",
      "@> 223 modes were calculated in 0.08s.\n",
      "@> Calculating covariance matrix\n",
      "@> Covariance matrix calculated in 0.0s.\n",
      "@> Calculating perturbation response\n",
      "@> Perturbation response matrix calculated in 0.0s.\n",
      "@> Perturbation response scanning completed in 0.0s.\n",
      "@> Hessian was built in 0.45s.\n",
      "@> 666 modes were calculated in 0.13s.\n",
      "@> Calculating covariance matrix\n",
      "@> Covariance matrix calculated in 0.0s.\n",
      "@> Calculating perturbation response\n",
      "@> Perturbation response matrix calculated in 0.0s.\n",
      "@> Perturbation response scanning completed in 0.0s.\n",
      "@> Calculating stiffness matrix.\n",
      "@> Stiffness matrix calculated in 0.24s.\n",
      "@> The range of effective force constant is: 4.698616287570003 to 26.31458794738374.\n",
      "@> Kirchhoff was built in 0.02s.\n",
      "@> 455 modes were calculated in 0.15s.\n",
      "@> Calculating covariance matrix\n",
      "@> Covariance matrix calculated in 0.0s.\n",
      "@> Calculating perturbation response\n",
      "@> Perturbation response matrix calculated in 0.0s.\n",
      "@> Perturbation response scanning completed in 0.0s.\n",
      "@> Hessian was built in 0.40s.\n",
      "@> 1362 modes were calculated in 0.65s.\n",
      "@> Calculating covariance matrix\n",
      "@> Covariance matrix calculated in 0.1s.\n",
      "@> Calculating perturbation response\n",
      "@> Perturbation response matrix calculated in 0.0s.\n",
      "@> Perturbation response scanning completed in 0.1s.\n",
      "@> Calculating stiffness matrix.\n",
      "@> Stiffness matrix calculated in 0.45s.\n",
      "@> The range of effective force constant is: 4.521514034162207 to 26.04557997532264.\n",
      "@> Pickle 'PDBfeatures-2FIM.pkl' saved.\n",
      "Analizing mutation site 2FIM:A 443: 100%|██████████| 2/2 [00:07<00:00]\n",
      "@> PDB features have been computed in 7.5s.\n",
      "@> Computing sequence properties from Pfam domains...\n",
      "Mapping SAV 'O00238 31 R H' to Pfam:   0%|          | 0/2 [00:00<?]@> Pickle 'UniprotMap-O00238.pkl' recovered.\n",
      "Mapping SAV 'O00294 496 A T' to Pfam:   0%|          | 0/2 [00:00<?]@> Pickle 'UniprotMap-O00238.pkl' saved.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' recovered.\n",
      "@> Pickle 'UniprotMap-O00294.pkl' saved.\n",
      "Mapping SAV 'O00294 496 A T' to Pfam: 100%|██████████| 2/2 [00:00<00:00]\n",
      "@> SAVs have been mapped on Pfam domains and sequence properties have been computed in 0.0s.\n",
      "@> Recovering EVmutation data...\n",
      "@> EVmutation scores recovered in 0.2s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([('O00294 496 A T', '2FIM A 443 A', 224, 1, 0.41869268, 0.2725254, 0.30990002, 0.00208266, 0.00289542, 0.0444308, 0.00296154, 0.00206255, 0.04027145, 0., 0.754,  1., -3.1479, -1.0065, 0.0662341, 0.3341, 0.09581726, 0.08719353, 0.08953744, 0.00398543, 0.00332497, 0.01278353, 0.00573205, 0.00480731, 0.01438932, 79., 78., 2.1440704, 0.813278  , 12.638108, 14.42894, 14.559032, -2.376),\n",
       "       ('O00238 31 R H', 'Unable to map SAV to PDB',   0, 0,        nan,       nan,        nan,        nan,        nan,       nan,        nan,        nan,        nan, 0., 1.634, nan, -2.4718, -2.461 , 0.0102818, 0.3508,        nan,        nan,        nan,        nan,        nan,        nan,        nan,        nan,        nan, nan, nan, 1.8702421, 0.23376623,       nan,      nan,       nan, -2.769)],\n",
       "      dtype=[('SAV_coords', '<U50'), ('Uniprot2PDB', '<U100'), ('PDB_length', '<i2'), ('true_label', '<i2'), ('ANM_MSF-chain', '<f4'), ('ANM_MSF-reduced', '<f4'), ('ANM_MSF-sliced', '<f4'), ('ANM_effectiveness-chain', '<f4'), ('ANM_effectiveness-reduced', '<f4'), ('ANM_effectiveness-sliced', '<f4'), ('ANM_sensitivity-chain', '<f4'), ('ANM_sensitivity-reduced', '<f4'), ('ANM_sensitivity-sliced', '<f4'), ('BLOSUM', '<f4'), ('Delta_PSIC', '<f4'), ('Delta_SASA', '<f4'), ('EVmut-DeltaE_epist', '<f4'), ('EVmut-DeltaE_indep', '<f4'), ('EVmut-mut_aa_freq', '<f4'), ('EVmut-wt_aa_cons', '<f4'), ('GNM_MSF-chain', '<f4'), ('GNM_MSF-reduced', '<f4'), ('GNM_MSF-sliced', '<f4'), ('GNM_effectiveness-chain', '<f4'), ('GNM_effectiveness-reduced', '<f4'), ('GNM_effectiveness-sliced', '<f4'), ('GNM_sensitivity-chain', '<f4'), ('GNM_sensitivity-reduced', '<f4'), ('GNM_sensitivity-sliced', '<f4'), ('SASA', '<f4'), ('SASA_in_complex', '<f4'), ('entropy', '<f4'), ('ranked_MI', '<f4'), ('stiffness-chain', '<f4'), ('stiffness-reduced', '<f4'), ('stiffness-sliced', '<f4'), ('wt_PSIC', '<f4')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = rh.exportTrainingData()\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('precomputed_features', training_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
